\documentclass[10.5pt]{article}
\usepackage{amsmath, amsfonts, amssymb,amsthm}
\usepackage[includeheadfoot]{geometry} % For page dimensions
\usepackage{fancyhdr}
\usepackage{enumerate} % For custom lists
\usepackage{xcolor}

\fancyhf{}
\lhead{MAT1600 hw2}
\rhead{Tighe McAsey - 1008309420}
\pagestyle{fancy}

% Page dimensions
\geometry{a4paper, margin=1in}

\theoremstyle{definition}
\newtheorem{pb}{}

% Commands:

\newcommand{\set}[1]{\{#1\}}
\newcommand{\abs}[1]{\lvert#1\rvert}
\newcommand{\norm}[1]{\lvert\lvert#1\rvert\rvert}
\newcommand{\tand}{\text{ and }}
\newcommand{\tor}{\text{ or }}
\newcommand{\floor}[1]{\lfloor#1\rfloor}

\begin{document}
    \begin{pb}\textbf{(Durrett 2.2.2)}
        We assume that \(r\) is a real valued function, i.e. that \(r(0) \in \mathbb{R}\) is not infinity. Then
        \begin{align*}
            0 \leq E\left[\left(\frac{S_n}{n}\right)^2\right] = \frac{1}{n^2}\left(\sum_1^n E X_j^2 + 2\sum_{\substack{1 \leq j \leq n \\ 1 \leq i < j}}EX_iX_j\right) &\leq \frac{1}{n^2}\left(nr(0) + 2\sum_{\substack{1 \leq j \leq n \\ 1 \leq i < j}}\abs{r(i-j)}\right) \\ &= \frac{r(0)}{n} + \frac{2}{n^2}\left(\sum_{j=1}^{n-1}(n-j)\abs{r(j)}\right)
        \end{align*}
        The term \(r(0)/n \to 0\), so it suffices to check that \(\frac{1}{n^2}\left(\sum_{j=1}^{n-1}(n-j)\abs{r(j)}\right) \to 0\). So take \(\epsilon > 0\), then for \(N \in \mathbb{N}\) we have \(j \geq N\) implies \(\abs{r(j)} < \epsilon/2\), then for \(M > N\),
        \begin{align*}
            \frac{1}{M^2}\sum_{j=N}^{M-1}(M-j)\abs{r(j)} < \frac{1}{M^2}(M-N-1)(M-N) \frac{\epsilon}{2} \leq \epsilon/2
        \end{align*}
        Now we can take \(M\) sufficiently large so that \(\frac{1}{M}\sum_1^{N-1}\abs{r(j)} < \epsilon/2\), then combining these inequalities, for any \(K \geq M\) we get
        \begin{align*}
            \frac{1}{K^2}\sum_1^{K-1}(K-j)\abs{r(j)} &= \frac{1}{K^2}\sum_N^{K-1}(K-j)\abs{r(j)} + \frac{1}{K^2}\sum_1^{N-1}(K-j)\abs{r(j)} < \epsilon/2 + \frac{1}{K}\sum_1^{N-1}\abs{r(j)} \\
            & \leq \epsilon/2 + \frac{1}{M}\sum_1^{N-1}\abs{r(j)} < \epsilon
        \end{align*}
        So indeed by squeeze theorem we find that \(E\left[\left(\frac{S_n}{n}\right)^2\right] \overset{L^2}{\to} 0\), and \(L^2\) convergence implies convergence in probability. \qed
    \end{pb}
    \begin{pb}\textbf{(Durrett 2.2.8)}
        In order to invoke the weak law of traingular arrays, we need to check the conditions.
        
        \textbf{(i) - \(\mathbf{\sum_1^n P(X_k>b_n) \to 0}\).}
        \begin{align*}
            P(X_k > b_n) &= \sum_{m(n)}^\infty p_j = \leq 2^{-m(n)}\sum_0^\infty \frac{1}{2^j(m(n)+j+1)(m(n)+j)} \leq 2^{-m(n)}m(n)^{-3/2}\frac{1}{\sqrt{m(n)}}\sum_0^\infty 2^{-j} \\&< \frac{2}{n\sqrt{m(n)}}
        \end{align*}
        So that
        \begin{align*}
            \sum_1^n P(X_k > b_n) < \frac{2}{\sqrt{m(n)}} \overset{n\to\infty}{\longrightarrow} 0
        \end{align*}

        \textbf{(ii) - \(\mathbf{\frac{1}{b_n^2}\sum_1^nE\overline{X}_{n,k}^2 \to 0}\)}, where \(\overline{X}_{n,k} = X_k1_{\set{\abs{X_k}\leq b_n}}\)
        \begin{align*}
            E\overline{X}_{n,k}^2 &= \left(\sum_1^{2^{m(n)}}p_k(2^k-1)-p_0\right)^2 = p_0^2 - 2p_0\left(\sum_1^{2^{m(n)}}\frac{1}{k(k+1)}-p_k\right) + \left(\sum_1^{2^{m(n)}}p_k(2^k-1)\right)^2 \\
            &\leq p_0^2 + \left(\sum_1^{2^{m(n)}}\frac{1}{k(k+1)} - p_k\right)^2 \leq p_0^2 + \left(\sum_1^{2^{m(n)}}\frac{1}{k(k+1)}\right)^2 \leq 1 + 1 = 2
        \end{align*}
        And hence,
        \begin{align*}
            \frac{1}{b_n^2}\sum_1^nE\overline{X}_{n,k}^2 \leq \frac{2n}{b_n^2} = \frac{2n}{2^{-2m(n)}} < \frac{2m(n)^{3/2}}{2^{-m(n)}} \overset{n \to \infty}{\longrightarrow} 0
        \end{align*}
        Now we want to obtain workable expressions to explain the asymptotic behaviour, we obtain the following expression for \(\sum_1^n E \overline{X}_{n,k}\) (here I not writing the dependence of \(m\) on \(n\) out of laziness)
        \begin{align*}
            \sum_1^n E \overline{X}_{n,k} &= n\left(-p_0 + \sum_1^m (2^k - 1)p_k\right) = n\left(\sum_1^m \frac{1}{k(k+1)} - 1 - \sum_1^m \frac{1}{2^k k(k+1)} + \sum_1^\infty \frac{1}{2^k k(k+1)}\right) \\ &= -n\left(1 - \frac{1}{m+1} - 1 + \sum_{m+1}^\infty \frac{1}{2^k k(k+1)}\right) = \frac{-n}{m+1} +n\sum_{m+1}^\infty \frac{1}{2^kk(k+1)}
        \end{align*}
        This gives us the following inequalities:
        \begin{align}
            \frac{-n}{m(n)+1} \leq \sum_1^n E \overline{X}_{n,k} \leq \frac{-n}{m(n)+1} + \frac{n}{(m(n)+1)(m(n)+2)}\sum_{m(n)+1}^\infty 2^{-k} \leq \frac{-n}{m(n)+1} + \frac{n}{m(n)^22^{m(n)}}
        \end{align}
        We also have that \(2n > 2^mm^{3/2}>n\) which gives us
        \begin{align}
            1 + \log n > m + \frac32 \log m > \log n
        \end{align}
        so that \(m \sim \log n\) and \(\sum_1^n E \overline{X}_{n,k} \sim \frac{-n}{m} \sim \frac{-n}{\log n}\) and \(b_n = m^{-3/2}n \sim n(\log n)^{-3/2}\). The weak law for triangular arrays applied here gives us that
        \begin{align*}
            \frac{S_n + \frac{n}{\log n}}{n(\log n)^{-3/2}} \overset{P}{\longrightarrow} 0 \implies \frac{S_n + \frac{n}{\log n}}{n\log n} \overset{P}{\longrightarrow} 0
        \end{align*}
        this can be rewritten as \(\frac{S_n}{\frac{n}{\log n}} \overset{P}{\to} -1\) \qed
    \end{pb}
    \begin{pb}\textbf{(Durrett 2.3.6)}

        \textbf{(Lemma)} If \(d(x,y)\) is a metric, then \(\frac{d(x,y)}{1+d(x,y)}\) is a metric, in particular \(D(x,y) = \frac{\abs{x-y}}{1 + \abs{x-y}}\) is a metric on \(\mathbb{R}\).
        \begin{proof}
            Symmetry, nonnegativity and \(\frac{d(x,y)}{1+d(x,y)} = 0 \iff x = y\) are obvious, as for triangle inequality, notice that \(\frac{x}{1+x}\) is an increasing function on \([0,\infty)\), and hence
            \begin{align*}
                \frac{d(x,z)}{1+d(x,z)} \leq \frac{d(x,y) + d(y,z)}{1+d(x,y) + d(y,z)} \leq \frac{d(x,y)}{1+d(x,y)} + \frac{d(y,z)}{1+d(y,z)}
            \end{align*}
        \end{proof}

        \textbf{(a)} Now to prove the main statement, firstly symmetry is obvious. If \(X = Y\) a.s. then \(\abs{X-Y}\) = 0 a.s. So that \(d(X,Y) = E[0] = 0\), and if this is not the case, then there exists some \(\epsilon > 0\) and some positive measure set \(S\) with \(\abs{X - Y} > \epsilon\) on \(S\) (to see this we can write \(\set{X \neq Y} = \bigcup_1^\infty \set{\abs{X-Y} > \frac{1}{n}}\), so that one of these sets in the countable union must have positive measure), then it follows that
        \begin{align*}
            d(X,Y) \geq E[\frac{\abs{X-Y}}{1+\abs{X-Y}}1_S] \geq \frac{\epsilon}{1 + \epsilon}P(S) > 0
        \end{align*}
        Now triangle inequality simply follows from pointwise triangle inequality on \(\mathbb{R}\) from the lemma, i.e. for all \(\omega\) we have \(D(X(\omega),Z(\omega)) \leq D(X(\omega),Y(\omega)) + D(Y(\omega),Z(\omega))\), so that
        \begin{align*}
            d(X,Z) = E[D(X,Z)] \leq E[D(X,Y) + D(Y,Z)] = E[D(X,Y)] + E[D(Y,Z)] = d(X,Y) + d(Y,Z)
        \end{align*} \qed

        \textbf{(b)} First suppose that \(X_n \to X\) in probability, then for some \(N\) we have for any \(n \geq N\) that \(P(\abs{X - X_n} \geq \epsilon) < \epsilon\), so for \(n \geq N\) we have
        \begin{align*}
            d(X,X_n) &\leq E[D(X,X_n)1_{\abs{X-X_n} \geq \epsilon} + D(X,X_n)1_{\abs{X-X_n} < \epsilon}] \\
            &\leq P(\abs{X-X_n} \geq \epsilon) + \frac{\epsilon}{1 + \epsilon} \leq 2 \epsilon
        \end{align*}
        Where here we used in the second inequality that \(D(X,X_n) \leq 1\) and \(P(\abs{X - X_n} < \epsilon) \leq 1\). To prove the converse, assume that \(X_n \not \to X\) in probability, then for some \(\epsilon > 0\), and any \(N\) we can find \(n \geq N\) so that \(P(\abs{X - X_n} \geq \epsilon) \geq \epsilon\), so letting \(N\) be arbitrary, we choose such an \(n\), then
        \begin{align*}
            d(X,X_n) \geq P(\abs{X - X_n} \geq \epsilon)\frac{\epsilon}{1+\epsilon} = \frac{\epsilon^2}{1+\epsilon} > 0
        \end{align*}
        so that we dont get metric convergence. \qed
    \end{pb}
    \begin{pb}\textbf{(Durrett 2.3.15)}
        Note that although the sets of probability one which properties are proven to hold may be different, we may intersect them to obtain another set of probability one in which all the properties hold, so I won't worry too much about what the sets of probability \(1\) are in the proof.

        \textbf{(i)} \(P(X_n > \log n) = \frac{1}{n}\), so that \(\sum_1^\infty P(X_n > \log n) = \infty\), by the second Borel Cantelli lemma, we get that \(P(X_n/\log n > 1 \text{ i.o.}) = P(X_n > \log n \text{ i.o.}) = 1\), so that \(\limsup \frac{X_n}{\log n} \geq 1\) almost surely. To prove that \(\limsup \frac{X_n}{\log n} \leq 1\) on a set of probability \(1\), it suffices to prove that for each natural number \(m\), there is a set of probability \(1\) so that \(\frac{X_n}{\log n} \leq 1 + \frac{1}{m}\) for all but finitely many \(n\), then to take the intersection of all these sets which will still have probability \(1\). This of course will be true if we can show that for any \(\epsilon > 0\) we have \(P(\frac{X_n}{\log n} > 1 + \epsilon \text{ i.o.}) = 0\), this follows from Borel Cantelli lemma since \(P(\frac{X_n}{\log n} > 1 + \epsilon \text{ i.o.}) = \frac{1}{n^{1 + \epsilon}}\), so that \(\sum_1^\infty P(X_n/\log n > 1 + \epsilon) < \infty\). \qed

        \textbf{(ii)} We have that \(\lim \frac{M_n}{\log n} = 1\) on \(\bigcap_{m=1}^\infty \set{\frac{M_n}{\log n} \leq 1 - \frac{1}{m} \text{ i.o.}}^c \bigcap_{m=1}^\infty \set{\frac{M_n}{\log n} > 1 + \frac{1}{m} \text{ only finitely often}}\), so showing each of the sets in the intersection have probability \(1\) will complete the proof. Therefore it is sufficient to show that for any \(\epsilon > 0\), \(P(\frac{M_n}{\log n} \leq 1 - \epsilon \text{ i.o.}) = 0\) and \(\set{\frac{M_n}{\log n} > 1 + \epsilon \text{ only finitely often}} \supset \set{x \mid \limsup \frac{X_n}{\log n} = 1}\), the latter has probability one by (i). We check the first condition with the Borel-Cantelli lemma
        \begin{align*}
            P\left(\frac{M_n}{\log n} \leq 1 - \epsilon\right) &= P\left(\bigcap_1^n \set{\frac{X_k}{\log n} \leq 1 - \epsilon}\right) = \prod_1^n P\left(X_k/\log n \leq 1 - \epsilon\right) \\
            &= \prod_1^n 1 - P\left( X_k/\log n > 1 - \epsilon\right) = \prod_1^n 1 - \frac{1}{n^{1-\epsilon}}
        \end{align*}
        Now we can apply the inequality \(1-x \leq e^{-x}\) to find that
        \begin{align*}
            P\left(\frac{M_n}{\log n} \leq 1 - \epsilon\right) \leq \exp\left(\frac{-1}{n^{1-\epsilon}}n\right) = \exp(-n^\epsilon)
        \end{align*}
        To apply Borel-Cantelli, we want to check that \(\sum_1^\infty \exp(-n^\epsilon) < \infty\), since \(\exp(-x^\epsilon)\) is monotone decreasing, we can apply the integral test.
        \begin{align*}
            \int_1^\infty \exp(-x^\epsilon)dx = \frac{1}{\epsilon}\int_1^\infty e^{-u}u^{1/\epsilon - 1}du \leq \frac{1}{\epsilon}\int_1^\infty e^{-u}du = 1/\epsilon
        \end{align*}
        So that
        \begin{align*}
            \sum_1^\infty P\left(\frac{M_n}{\log n} \leq 1 - \epsilon\right) \leq \sum_1^\infty \exp(-n^\epsilon) < \infty
        \end{align*}
        which gives us \(P\left(\frac{M_n}{\log n} \leq 1 - \epsilon \text{ i.o.}\right) = 0\) from Borel-Cantelli. Now we will prove the set inclusion above to finish the proof, let \(x \in \set{x \mid \limsup \frac{X_n}{\log n} = 1}\), then \(\frac{X_n}{\log n}(x) > 1 + \epsilon\) only finitely many times. So take \(N\) such that for all \(n \geq N\) we have \(\frac{X_n}{\log n}(x) \leq 1 + \epsilon\), since \(\log n \to \infty\) we can take \(M \geq N\) so that \(\log M > X_k(x)\) when \(1 \leq k \leq N\), then for any \(n > \max{N,M}\) we find that
        \begin{align*}
            \frac{\max_{1 \leq k \leq n}X_k(x)}{\log n} = \max\left\{\frac{\max_{1 \leq k \leq N}X_k(x)}{\log n}, \frac{\max_{N \leq k \leq n}X_k(x)}{\log n}\right\}
        \end{align*}
        The first term we have \(\frac{X_k}{\log n}(x) \leq \frac{X_k}{\log M}(x) < 1\) since \(1 \leq k \leq N\), and the second term we have \(\frac{X_k}{\log n}(x) \leq \frac{X_k(x)}{\log k} \leq 1 + \epsilon\) since \(k \geq N\). This completes the proof that \(\set{\frac{M_n}{\log n} > 1 + \epsilon \text{ only finitely often}} \supset \set{x \mid \limsup \frac{X_n}{\log n} = 1}\), and hence the former has probability \(1\) completing the proof of \(\lim \frac{M_n}{\log n} = 1\) a.s. \qed
    \end{pb}
    \begin{pb} \textbf{(Durrett 2.3.19)}
        We first note the property of poisson distributions \(Y_k \sim \text{Poisson}(\lambda_k)\) implies that \(\sum_1^n Y_k \sim \text{Poisson}(\sum_1^n \lambda_k)\), the naive proof technique runs into an issue if the means are not bounded, so we will use this to reduce to the case of bounded means.

        Assume for now we can write \(X_n = \sum_{j=1}^{\floor{\lambda_n} + 1} X_{n,j}\) with \(X_{n,j}\) independent and poisson with mean at most \(1\). Then writing \(Y_1 = X_{1,1}, Y_2 = X_{1,2}, \hdots, Y_{\floor{\lambda_1} + 1} = X_{1,\floor{\lambda_1} + 1}, Y_{\floor{\lambda_1} + 2} = X_{2,1}, \hdots\) we find that \(\sum_1^n X_k\) is a subsequence of \(\sum_1^n Y_k\) and hence it suffices to prove the result in the case that \(\lambda_k \leq 1\) for all \(k\). It is actually very annoying to write down this decomposition and verify that it works, so I will delay that until after I have completed the proof.

        Now assuming that \(\lambda_k \leq 1\), we use in one of the equalities that Poisson distribution has equal variance and expectation.
        \begin{align*}
            P\left(\frac{\abs{S_{n} - E[S_{n}]}}{E[S_n]} > \epsilon\right) \leq \frac{1}{\epsilon^2}\frac{\text{Var}\,S_n}{E[S_n]^2} = \frac{1}{\epsilon^2}\frac{E[S_n]}{E[S_n]^2} = \frac{1}{\epsilon^2}\frac{1}{\sum_1^n \lambda_k}
        \end{align*}
        Then since \(\sum_1^n \lambda_k = E[S_n] \to \infty\), we can for each natural number \(k\) define \(n_k := \inf\set{n \mid E[S_n] > k^2}\), our assumption on the variances being small thus gives us that \(k^2 + 1 \geq E S_{n_k} > k^2\), then by the computation above applied to this subsequence, we find that
        \begin{align*}
            P\left(\frac{\abs{S_{n_k} - E[S_{n_k}]}}{E[S_{n_k}]} > \epsilon\right) \leq \frac{1}{k^2 \epsilon^2}
        \end{align*}
        which implies that \(\sum_{k=1}^\infty P\left(\frac{\abs{S_{n_k} - E[S_{n_k}]}}{E[S_{n_k}]} > \epsilon\right) \leq \frac{1}{\epsilon^2}\sum_1^\infty 1/k^2 < \infty\), applying Borel-Cantelli we get that
        \begin{align*}
            \frac{\abs{S_{n_k} - E[S_{n_k}]}}{E[S_{n_k}]} \leq \epsilon \text{ a.s.}
        \end{align*}
        and since \(\epsilon\) was arbitrary we get that \(\frac{\abs{S_{n_k}}}{E[S_{n_k}]} \to 1\) almost surely (here we can apply it for \(\epsilon = 1/m\) for \(m = 1,2,\hdots\), then intersect all these sets of probability 1). Now let \(\mathcal{S} = \set{\frac{\abs{S_{n_k}}}{E[S_{n_k}]} \to 1}\), then for any \(z \in \mathcal{S}\) and any \(n \geq n_1\), we can take \(n_k\) such that \(n_k \leq n \leq n_{k+1}\) so that
        \begin{align*}
            \frac{S_{n_k}}{E[S_{n_{k+1}}]}(z) \leq \frac{S_n}{E[S_n]}(z) \leq \frac{S_{n_{k+1}}}{E[S_{n_k}]}(z)
        \end{align*}
        then letting \(c_k := \frac{E[S_{n_{k+1}}]}{E[S_{n_{k}}]}\), we have \(c_k \geq 1\) and \(c_k \leq \frac{(k+1)^2 + 1}{k^2}\), so that \(\lim_{k\to \infty}c_k = 1\). Then we find that on \(\mathcal{S}\), we have both
        \begin{align*}
            \frac{S_{n_k}}{E[S_{n_{k+1}}]} &= \frac{S_{n_k}}{E[S_{n_k}]}c_k^{-1} \to 1 \\
            \frac{S_{n_{k+1}}}{E[S_{n_{k}}]} &= \frac{S_{n_{k+1}}}{E[S_{n_{k+1}}]}c_k \to 1
        \end{align*}
        It follows that on \(\mathcal{S}\) we have
        \begin{align*}
            1 &= \lim_{k\to\infty}\frac{S_{n_k}}{E[S_{n_{k+1}}]} \leq \liminf \frac{S_n}{E[S_n]} \\
            1 &= \lim_{k\to\infty}\frac{S_{n_{k+1}}}{E[S_{n_{k}}]} \geq \limsup \frac{S_n}{E[S_n]}
        \end{align*}
        so that \(\frac{S_n}{E[S_n]} \to 1\) on \(\mathcal{S}\) and hence almost surely, as desired.

        Now to prove we can break up our random variable as exploited in the proof, we first prove it for integer mean using induction, then notice the proof of the induction step suffices to prove the result for non-integer mean (I will not do this explicitly since the proof is the exact same but much uglier). So suppose that \(X\) is a poisson random variable with mean \(\lambda \in \mathbb{Z}_{>0}\), if \(\lambda = 1\), then we are done, so we have the base case and we can assume that any Poisson distributed random variable mean \(\lambda - 1\) can be written as a sum of independent Poisson distributed random variables with mean \(1\). Now let \(\set{Y_j}_{j=1}^\infty\) be i.i.d with \(P(Y_j = 1) = 1/\lambda\) and \(P(Y_j = 0) = \frac{\lambda - 1}{\lambda}\). We now take \(X_1(x) = \sum_{j=1}^{X(x)} Y_j(x)\) and \(X_2 = X - X_1\). We need to check that \(X_1 \sim \text{Poisson}(1)\), \(X_2 \sim \text{Poisson}(\lambda-1)\) and that \(X_1,X_2\) are independent, then we will be done by induction. First we check that \(X_1\) is Poisson, this follows from
        \begin{align*}
            P(X_1 = k) &= \sum_{\ell \geq k} P(X = \ell)P\left(\sum_1^\ell Y_j = k\right) = \sum_{\ell \geq k} P(X = \ell)\binom{\ell}{k}\left(\frac{1}{\lambda}\right)^k\left(\frac{\lambda - 1}{\lambda}\right)^{\ell - k} \\
            &= \sum_{\ell \geq k} \frac{\lambda^\ell e^{-\lambda}}{\ell!}\binom{\ell}{k}\frac{(\lambda-1)^{\ell-k}}{\lambda^\ell} = \sum_{\ell \geq k} \frac{e^{-\lambda}}{k!(\ell-k)!}(\lambda-1)^{\ell - k} = \frac{e^{-\lambda}}{k!}\sum_0^\infty \frac{(\lambda - 1)^j}{j!} \\
            &= \frac{e^{-\lambda}}{k!} e^{\lambda - 1} = \frac{e^{-1}}{k!}
        \end{align*}
        So \(X_1 \sim \text{Poisson}(1)\), to work with \(X_2\), we can take the Bernoulli random variables \(1 - Y_j\), with parameter \(\frac{\lambda - 1}{\lambda}\), and notice \(X_2(x) = \sum_{j=1}^{X(x)} (1 - Y_j)(x)\), carrying out a similar computation to above we find that
        \begin{align*}
            P(X_2 = k) = e^{-\lambda}\sum_{\ell \geq k}\frac{(\lambda - 1)^k}{k!(\ell-k)!} = \frac{e^{-\lambda}(\lambda-1)^k}{k!}\sum_0^\infty 1/j! = \frac{e^{1-\lambda}(\lambda-1)^k}{k!}
        \end{align*}
        so \(X_2 \sim \text{Poisson}(\lambda - 1)\). To check independence, we once again compute directly
        \begin{align*}
            P(X_1 = k, X_2 = r) &= \sum_{\ell = 0}^\infty P(X_1 = k, X_2 = r, X = \ell)P(X = \ell) \\
            &= P(X_1 = k, X_2 = r, X = k+r)P(X = k + r) \\
            &= \binom{k+r}{k}\frac{1}{\lambda^k}\left(\frac{\lambda - 1}{\lambda}\right)^r \frac{e^{-\lambda}\lambda^{k+r}}{(k+r)!} = e^{-\lambda}\frac{(\lambda-1)^r}{k!r!} \\
            &= P(X_1 = k)P(X_2 = r)
        \end{align*} \qed
    \end{pb}
    \begin{pb}\textbf{(Durrett 2.4.2)}
        Define random variables \(Y_n = \frac{\abs{X_n}}{\abs{X_{n-1}}}\), then since \(Y_n\) is independent of \(X_1,\hdots,X_{n-1}\) we get that \(Y_1,\hdots,Y_n\) are pairwise independent, and hence so are \(\log Y_j\) for each \(j\), moreover since the \(Y_j\) are identically distributed in the problem statement, we have \(\set{\log Y_j}_{j=1}^\infty\) are identically distributed and pairwise independent. Now we can compute
        \begin{align*}
            E\abs{\log Y_1} &= \frac{1}{\pi}\int_0^{2\pi}\int_0^1 \abs{\log r}rdrd\theta = 2\int_0^1 \abs{\log r}rdrd\theta = 2\int_0^1 -x\log x dx \\
            &= -2 \left[\frac{x^2}{2}\log x - \int \frac{x}{2}dx\right]_0^1 = \frac{1}{2} < \infty
        \end{align*}
        So we may apply the strong law of large numbers to \(\set{\log Y_j}_1^\infty\), which gives us
        \begin{align*}
            \frac{\sum_1^n \log Y_j}{n} \overset{\text{a.s.}}{\longrightarrow} E[\log Y_1] = \frac{1}{\pi}\int_0^{2\pi}\int_0^1 (\log r) r drd\theta = 2\int_0^1 x\log x dx = -\frac12
        \end{align*}
        Now we are done with \(c = -\frac12\), since
        \begin{align*}
            \abs{X_n} = \prod_1^n Y_n \implies \log \abs{X_n} = \sum_1^n \log Y_j
        \end{align*}
        so that \(n^{-1}\log \abs{X_n} = \frac{1}{n}\sum_1^n \log Y_j \to -\frac12\) a.s. \qed 
    \end{pb}
    \begin{pb}\textbf{(Durrett 3.2.6)}
        We first show that \(\rho(F,G)\) is real valued, since taking \(\epsilon = 1\), we get the desired inequalities so that the inf is finite. Now to see its a metric, suppose that \(F = G\) in distribution, then for any \(\epsilon > 0\), we have \(F(x- \epsilon) -\epsilon \leq F(x - \epsilon) \leq G(x) \leq F(x + \epsilon) \leq F(x + \epsilon) + \epsilon\), since \(F = G\) except at jump discontinuities, but this takes \(F\) before and after each jump discontinuity, hence \(\rho(F,G) = 0\). Now suppose \(F \neq G\), then there is a point \(x_0\) such that \(F\) is continuous at \(x_0\), and \(F(x_0) \neq G(x_0)\) (WLOG assume that \(F(x_0) > G(x_0)\), otherwise we can do the same proof for \(F(x_0) < G(x_0)\)). Then defining \(r = F(x_0) - G(x_0) > 0\), there is some \(\delta > 0\), such that \(\abs{x-x_0} < \delta\) implies \(\abs{F(x_0) - F(x)} < r/2\), letting \(\epsilon = \min\set{\delta/2,r/4}\) we get that
        \begin{align*}
            F(x_0 - \epsilon) - \epsilon \geq F(x_0) - \abs{F(x_0) - F(x_0 - \epsilon)} - \epsilon \geq F(x_0) - \frac{3r}{4} > G(x_0)
        \end{align*}
        so that \(\rho(F,G) \geq \min\set{\delta/2,r/4} > 0\). Now we can prove symmetry, suppose we have for all \(x\),
        \begin{align*}
            F(x - \epsilon) - \epsilon \leq G(x) \leq F(x + \epsilon) + \epsilon
        \end{align*}
        then taking the substitution of variables, \(u = x - \epsilon\) and \(v = x + \epsilon\), we find for all \(u\) and \(v\) that
        \begin{align*}
            F(u) - \epsilon \leq G(u + \epsilon) \tand G(v - \epsilon) \leq F(v) + \epsilon
        \end{align*}
        Since this holds for all \(u,v\) we find that for any \(x\),
        \begin{align*}
            G(x - \epsilon) - \epsilon \leq F(x) \leq G(x + \epsilon) + \epsilon
        \end{align*}
        this gives us the inequality \(\rho(G,F) \leq \rho(F,G)\), but the same argument gives us \(\rho(F,G) \leq \rho(G,F)\) so they are equal. Now for the triangle inequality, let \(a > \rho(F,G)\) and \(b > \rho(G,H)\), then
        \begin{align*}
            F(x - b - a) - a - b \leq G(x - b) - b \leq H(x) \leq G(x + b) + b \leq F(x + b + a) + a + b
        \end{align*}
        so that \(a + b \geq \rho(F,H)\), since this holds for all such \(a,b\) we find that \(\rho(F,H) \leq \rho(F,G) + \rho(G,H)\).

        Now assume that \(\rho(F_n,F) \to 0\), let \(\epsilon > 0\) and suppose that \(x_0\) is a point of continuity for \(F\) so there exists some \(\delta > 0\) with \(F\vert_{B_\delta(x_0)} \in B_{\epsilon/2}(F(x_0)) \), then we can define \(a_n = \rho(F_n,F) + 1/n\), so that \(a_n \to 0\), then for \(a_n\) sufficiently large we have \(N\) so that \(n \geq N\) implies \(a_n < \min\set{\delta,\epsilon/2}\), so that
        \begin{align*}
            F(x_0) - \epsilon < F(x_0 - a_n) - a_n \leq F_n(x_0) \leq F(x_0 + a_n) + a_n < F(x_0) + \epsilon
        \end{align*}
        therefore \(F_n(x) \to F(x)\) at all points where \(F\) is continuous, so we get convergence in distribution. Conversely, suppose that \(F_n\) converges weakly to \(F\), then let \(\epsilon > 0\), and denote \(\mathcal{C} := \set{x \mid F \text{ is continuous at }x}\). Then \(\mathcal{C}^c\) is countable, so that \(\mathcal{C}\) is dense, define \(y' = \sup\set{x \mid F(x) < \epsilon}\), then we can take some \(y < y'\) with \(y \in \mathcal{C}\), similarly letting \(z' = \inf\set{x \mid F(x) > 1 - \epsilon}\), we can take some \(z > z'\) with \(z \in \mathcal{C}\). Now using density, we can choose (here \(t\) is some positive number \(\leq \epsilon/2\))
        \begin{align*}
            &x_0 = y \\
            &x_1 \in (y,y+\epsilon/2) \cap \mathcal{C} \\
            &x_2 \in (y+\epsilon/2,y+\epsilon) \cap \mathcal{C} \\
            &\vdots \\
            &x_{N-1} \in (z - t, z - t + \epsilon/2) \cap \mathcal{C} \\
            &x_N \in (z - t + \epsilon/2, z - t + \epsilon) \cap \mathcal{C}
        \end{align*}
        It follows that \(\bigcup_1^N [x_{j-1},x_j] = [x_0,x_N]\) with \(F(x_0) < \epsilon\), \(F(x_N) > 1 - \epsilon\), \(F\) continuous at each \(x_j\), and \(\abs{x_j - x_{j-1}} < \epsilon\). For each \(x_j\) there is some \(M_j\) so that \(n \geq M_j\) implies \(\abs{F(x_j) - F_n(x_j)} < \epsilon\), letting \(M = \max_{0 \leq j \leq N}{M_j}\), we find that letting \(n \geq M\) we have for \(x \leq x_0\) that \(F_n(x) < F(x_0) + \epsilon < 2 \epsilon\), so that
        \begin{align*}
            F(x - 2 \epsilon) - 2 \epsilon \leq 0 \leq F_n(x) \leq 2 \epsilon \leq F(x + 2 \epsilon) + 2 \epsilon
        \end{align*}
        and similarly for \(x \geq x_N\), we have 
        \begin{align*}
            F(x - 2 \epsilon) - 2 \epsilon  \leq 1 - 2 \epsilon \leq F_n(x_N) \leq F_n(x) \leq 1 \leq F(x + 2 \epsilon) + 2 \epsilon
        \end{align*}
        Now if \(x_0 < x < x_N\), there is some \(j\) such that \(x \in [x_{j-1},x_j]\), it follows by construction that
        \begin{align*}
            &F(x - 2\epsilon) - 2 \epsilon \leq F(x_{j-1}) - 2 \epsilon \leq F_n(x_{j-1}) \leq F_n(x) \\
            &F_n(x) \leq F_n(x_j) \leq F(x_j) + 2 \epsilon \leq F(x_j + 2 \epsilon) + 2 \epsilon
        \end{align*}
        which suffices to show that for \(n \geq M\) we have \(\rho(F,F_n) \leq 2 \epsilon\), and since \(\epsilon\) was arbitrary we conclude that \(\rho(F_n,F) \to 0\). \qed
    \end{pb}
    \begin{pb}
        
    \end{pb}
\end{document}