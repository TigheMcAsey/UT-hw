\documentclass[10.5pt]{article}
\usepackage{amsmath, amsfonts, amssymb,amsthm}
\usepackage[includeheadfoot]{geometry} % For page dimensions
\usepackage{fancyhdr}
\usepackage{enumerate} % For custom lists
\usepackage{xcolor}

\fancyhf{}
\lhead{MAT1600 Exercises}
\rhead{Tighe McAsey - 1008309420}
\pagestyle{fancy}

% Page dimensions
\geometry{a4paper, margin=1in}

\theoremstyle{definition}
\newtheorem{pb}{}

% Commands:

\newcommand{\set}[1]{\left\{#1\right\}}
\newcommand{\abs}[1]{\left\vert#1\right\vert}
\newcommand{\norm}[1]{\lvert\lvert#1\rvert\rvert}
\newcommand{\tand}{\text{ and }}
\newcommand{\tor}{\text{ or }}
\newcommand{\var}{\text{Var}}

\begin{document}
    \begin{pb}\textbf{(Durrett 2.5.3)}
        Define \(Y_n = \frac{\sin(n\pi t)}{n}X_n\), then since \(X_n \sim N(0,1)\) we get \(\var\, Y_n = \frac{\abs{\sin(n\pi t)}}{n^2} \leq 1/n^2\). Then since \(\sum_1^\infty \text{Var}\,Y_n < \infty\) and \(Y_1,Y_2,\hdots\) are independent, we get from the consequence of Kolmogorov maximal inequality that \(\sum_1^\infty Y_n\) converges almost surely. \qed
    \end{pb}
    \begin{pb}\textbf{(Durrett 2.5.6)}
        We can use the Kolmogorov \(3\)-series test. First notice,
        \begin{align*}
            E[\psi(X_n)] = E\left[\abs{X_n}1_{\abs{X_n} > 1} + X_n^21_{\abs{X_n} \leq 1}\right]
        \end{align*}
        So in particular by the comparison test,
        \begin{align*}
            & \sum_1^\infty E\left[\abs{X_n}1_{\abs{X_n} > 1}\right] < \infty &\sum_1^\infty E\left[X_n^21_{\abs{X_n} \leq 1}\right] < \infty
        \end{align*}
        The latter is the series \(\sum_1^\infty \var(X_n1_{\abs{X_n} \leq 1})\), another of the series is relatively free, namely
        \begin{align*}
            \sum_1^\infty P(\abs{X_n} > 1) \leq \sum E\left[\abs{X_n}1_{\abs{X_n}>1}\right] < \infty
        \end{align*}
        Finally, we have \(\abs{E[X_n1_{\abs{X_n} > 1}]} \leq E\left[\abs{X_n}1_{\abs{X_n} > 1}\right]\), so that \(\sum_1^\infty E\left[X_n1_{\abs{X_n} > 1}\right]\) converges, since
        \begin{align*}
            0 = \sum_1^N E\left[X_n\right] = \sum_1^N E\left[X_n1_{\abs{X_n} > 1} + X_n1_{\abs{X_n \leq 1}}\right] = \sum_1^N E\left[X_n1_{\abs{X_n} > 1}\right] + \sum_1^N E\left[X_n1_{\abs{X_n \leq 1}}\right]
        \end{align*}
        we have
        \begin{align*}
            \sum_1^\infty E\left[X_n1_{\abs{X_n \leq 1}}\right] = - \sum_1^\infty E\left[X_n1_{\abs{X_n} > 1}\right] \in \mathbb{R}
        \end{align*}
        Thus \(X_n\) satisfy the hypothesis of the Kolmogorov \(3\)-series test and converge almost surely. \qed
    \end{pb}
    \begin{pb}\textbf{(Durrett 2.5.8)}
        Write \(Y = \log^+\abs{X_1}\) and assume first that \(EY = \infty\), then for any \(c > 0\), we find that
        \begin{align*}
            EY = \int P(Y>t)dt = \sum_1^\infty \int_{(n-1)c}^{nc} P(Y>t)dt \leq \sum_0^\infty cP(Y>nc)
        \end{align*}
        which in turn implies that \(\sum_1^\infty P(\log^+\abs{X_n} > nc) =  \sum_1^\infty P(Y > nc) = \infty\), therefore by Borel-Cantelli (ii), we find that \(P(X_n > e^{nc} \text{ i.o.}) = 1\), so for any \(c \neq 0\) we find that \(\abs{X_n}c^n > 1\) i.o. almost-surely. This of course implies that \(\sum_1^\infty \abs{X_n}c^n\) has radius of convergence zero almost surely.

        Now, conversely suppose \(E\log^+ \abs{X_n} < \infty\), and let \(0 < c < 1\), then choose \(\gamma > 0\) so that \(e^{\gamma}c < 1\) we get another layer cake estimate,
        \begin{align*}
            EY = \int P(Y>t)dt = \sum_1^\infty \int_{(n-1)\gamma}^{n\gamma} P(Y>t)dt \geq \sum_1^\infty \gamma P(Y>n\gamma)
        \end{align*}
        Since \(\sum_1^\infty \gamma P(Y>n\gamma) = \sum_1^\infty P(\log^+ \abs{X_n} > n\gamma)\) by Borel Cantelli, we have \(\abs{X_n} > e^{n\gamma}\) only finitely many times. Letting \(N\) so that \(n \geq N\) implies \(\abs{X_n} \leq e^{n\gamma}\), we get
        \begin{align*}
            \sum_1^\infty c^n\abs{X_n} \overset{\text{a.s.}}{\leq} \sum_1^{N-1}c^n\abs{X_n} + \sum_N^\infty (ce^{\gamma})^n < \infty
        \end{align*}
        So that the series converges a.s. for any \(c < 1\). Now letting \(c > 1\), we once again use Borel-Cantelli (ii), we first check the hypothesis
        \begin{align*}
            \sum_1^\infty P(\abs{X_n}c^n > 1) = \sum_1^\infty P(\abs{X_1} > \frac{1}{c^n}) = \infty
        \end{align*}
        Since \(\lim_{n\to\infty}P(\abs{X_1} > \frac{1}{c^n}) = P(X_1 \neq 0) > 0\) by assumption, it follows that \(P(\abs{X_n}c^n > 1 \text{ i.o.}) = 1\), so the series diverges a.s. for \(c > 1\). \qed
    \end{pb}
    \begin{pb}\textbf{(Durrett 3.2.9)}
        We can use a mesh to show that \(\norm{F - F_n}_u = 0\). Namely, let \(\epsilon > 0\), then we can choose \(x_0\) so that \(F(x_1) < \epsilon\) and \(y\) so that \(F(y) > 1 - \epsilon\), then \(F\) is uniformly continuous on \([x_0,y]\), so in particular there is some \(\delta > 0\) such that \(\abs{w - z} < \delta \implies \abs{F(w) - F(z)} < \epsilon\) on \([x_0,y]\), then take \(x_j = x_0 + j\frac{\delta}{2}\) for \(j \in \set{1,\hdots,N-1}\), so that \(y - \delta/2 < x_{N-1} \leq y\) and denote \(y = x_N\). Then take \(M_j\), so that \(n \geq M_j \implies \abs{F_n(x_j) - F(x_j)} < \epsilon\) and define \(M = \max_{0 \leq j \leq N}M_j\). It follows that for \(n \geq N\), we have for \(x \in (-\infty,x_0)\),
        \begin{align*}
            F_n(x) \leq F_n(x_0) < F(x_0) + \epsilon < 2 \epsilon
        \end{align*}
        and for \(x \in (x_N,\infty)\)
        \begin{align*}
            F_n(x) \geq F_n(x_N) > F(x_N) - \epsilon > 1- 2 \epsilon
        \end{align*}
        and on these ranges \(F(x) < \epsilon\) and \(F(x) > 1 - \epsilon\) respectively, meaning \(\abs{F(x) - F_n(x)} < 2 \epsilon\). Finally, suppose that \(x \in [x_{j-1},x_j]\) for some \(j\), then we have
        \begin{align*}
            F(x) - 2 \epsilon < F(x_{j-1}) - \epsilon \leq F_n(x_{j-1}) \leq F_n(x) \leq F_n(x_j) \leq F(x_j) + \epsilon < F(x) + 2 \epsilon
        \end{align*}
        So that \(\norm{F - F_n}_u < 2 \epsilon\) and since \(\epsilon\) was arbitrary, \(\norm{F -F_n}_u \to 0\). \qed
    \end{pb}
    \begin{pb}\textbf{(Durrett 3.2.12)}
        One direction is much easier, so first suppose that \(X_n \implies c\), then for any \(\epsilon > 0\), we have \(P(X_n \leq c - \epsilon) \to P(c \leq c - \epsilon) = 0\), and \(P(X_n > c + \epsilon) = 1 - P(X_n \leq c + \epsilon) \to 1 - P(c \leq c + \epsilon) = 0\), so that \[P(\abs{X_n - c} > \epsilon) \leq P(X_n > c + \epsilon) + P(X_n \leq c - \epsilon) \to 0\]

        For the (more general) converse, I should first mention that the DCT applies to convergence in probability (every subsequence has a subsequence converging almost surely which will have the same DCT limit) we can use the Portmanteau lemma, which gives equivalence to weak-* convergence on bounded continuous functions. Namely, we want to show that \(\int f d\mu_n \to \int f d\mu\) for all bounded continuous \(f\). Now letting \(f\) be bounded and continuous, we have that \(E\abs{f} < \infty\) so we can apply DCT, observe from the change of variables formula,
        \begin{align*}
            \int fd\mu_n = E[f\circ X_n] \overset{\text{DCT}}{\longrightarrow} E[f\circ X] = \int fd\mu
        \end{align*} \qed
    \end{pb}
    \begin{pb}\textbf{(Durrett 3.2.13) - Converging Together Lemma}
        We use the previous problem, \(Y_n \implies c\), so that \(Y_n\overset{P.}{\longrightarrow} c\) now we can write \(P(X \leq \omega) = P(X + c \leq \omega + c)\), if \(\mathcal{C}_X\) is the set of points where \(F_X\) is continuous, the set of points where \(F\) is continuous is \(\mathcal{C}_X + c\). Now let \(\epsilon > 0\), and \(x \in \mathcal{C}_X + c\), then choose \(\delta\) such that \(\abs{x-c-y} \leq \delta\) implies \(\abs{F(x-c) - F(y)} < \epsilon\) and \(x - c -\delta\) is a point of continuity for \(F\)
        \begin{align*}
            \set{X_n + Y_n \leq x} \supset \set{X_n \leq x - \delta - c}\cap\set{Y \in (c-\delta,c+\delta)}
        \end{align*}
        We can then write
        \begin{align*}
            &P(X_n + Y_n \leq x) \geq F_n(x - c - \delta) - P(Y_n \not \in (c - \delta, c + \delta)) \\
            &\geq F_X(x-c) - \abs{F_n(x-c-\delta) - F_X(x-c-\delta)} - \abs{F_X(x - c - \delta) - F_X(x-c)} - P(Y_n \not \in (c - \delta, c + \delta))
        \end{align*}
        Now taking \(n\) large enough, we get \(\abs{F_n(x-c-\delta) - F_X(x-c-\delta)} < \epsilon\) and \(P(Y_n \not \in (c - \delta, c + \delta)) = P(\abs{Y_n - c} \geq \delta) < \epsilon\) so that
        \begin{align*}
            P(X_n + Y_n \leq x) \geq F_X(x-c) - 3 \epsilon = F(x-c) - 3 \epsilon
        \end{align*}
        For the converse inequality,
        \begin{align*}
            P(X_n+Y_n \leq X) &= P(X_n + Y_n \leq x, \abs{Y_n - c} \geq \delta) + P(X_n + Y_n \leq x, \abs{Y_n - c} < \delta) \\
            &\leq P(\abs{Y_n - c} \geq \delta) + P(X_n \leq x - c - \delta) \leq F_X(x-c) + 3 \epsilon
        \end{align*}
        by the same inequalities as above \qed
    \end{pb}
    \begin{pb}\textbf{(Durrett 3.2.15)}
        We let \(Y_j \sim N(0,1)\), and \(\hat{X}^j_n = Y_j(\frac{n}{\sum_1^n Y_j^2})\), we will show later that \(\hat{X} \overset{\text{dist.}}{=} X\), and that \(\hat{X}_n^1 \implies N(0,1)\). Starting with the latter, \(Y_j^2\) are i.i.d. With \(E \abs{Y_j^2} = E Y_j^2 = \text{var}(Y_j) + (E Y_j)^2 = 1\), whence we can apply the strong law of large numbers to \(\frac{\sum_1^n Y_j^2}{n} \overset{\text{a.s.}}{\longrightarrow} 1\) so that \(\hat{X}_n^1 \overset{\text{a.s.}}{\longrightarrow} Y_1 \sim N(0,1)\).

        Now to see that \(\hat{X} \overset{\text{dist.}}{=} X\), we will show that \(\hat{X}\) is uniformly distributed on the sphere of radius \(\sqrt{n}\), first note that it indeed takes values on this sphere since \(\sqrt{\sum_1^n \left(Y_j\left(\frac{n}{\sum_1^n Y_k^2}\right)\right)^2} = \sqrt{n}\). Now we can use that the uniform distribution is the unique distribution which is preserved by rotation on the \(\sqrt{n}\)-sphere. Denoting \(Y = (Y_1,\hdots,Y_n)\), \(Y\) having distribution \(F\) can use characteristic functions to check that
        \begin{align*}
            \varphi_{AY}(t) = \int e^{it^T AY}dF = \int e^{i(A^Tt)^TY}dF = \varphi_Y(A^Tt) = e^{\norm{A^Tt}^2/2} = e^{\norm{t}^2/2} = \varphi_Y(t)
        \end{align*}
        So that \(Y = AY\), now \(A\hat{X} = \sqrt{n}\frac{AY}{\norm{Y}} = \sqrt{n}\frac{Y}{\norm{Y}}\). \qed
    \end{pb}
    \begin{pb} \textbf{(Durrett 1.7.5)}
        The following is used in the proof of Fourier inversion, namely \(\int_0^\infty \frac{\sin x}{x} = \frac{\pi}{2}\). Moreover the trick \(\frac{1}{x} = \int_0^\infty e^{-xy}dy\), then using Fubini is frequently useful for integration. Now on with the proof,

        Let \(I := \int_0^a e^{-xy}\sin x dx\), then
        \begin{align*}
            I &= \left[-\cos x e^{-xy} - y\int \cos x e^{-xy}\right]_{x=0}^a = \left[-\cos x e^{-xy} - ye^{-xy}\sin x - y^2\int e^{-xy}\sin x\right]_{x=0}^a \\
            &= 1 -\cos a e^{-ay} - ye^{-ay}\sin a + y^2 I
        \end{align*}
        So that \(I = \frac{1}{1 + y^2} - \cos a \frac{e^{-ay}}{1 + y^2} - \sin a \frac{ye^{-ay}}{1+y^2}\)
    \end{pb}
    Now it is easy to see \(\int_0^\infty Idy = \pi/2 - \cos a \int_0^\infty \frac{e^{-ay}}{1 + y^2}dy - \sin a \int_0^\infty \frac{ye^{-ay}}{1+y^2}\). Since Tolleni's theorem always holds for positive functions, we can check the assumptions of Fubini on \(\abs{e^{-xy}\sin x}\), using \(\abs{\sin x} \leq \abs{x}\)
    \begin{align*}
        \int_0^a \int_0^\infty \abs{e^{-xy}\sin x}dydx = \int_0^a \frac{\abs{\sin x}}{x}dx \leq a
    \end{align*}
    All of the work has been done, so now we can breeze through the proof
    \begin{align*}
        \int_0^a \frac{\sin x}{x} &= \int_0^a \int_0^\infty e^{-xy}\frac{\sin x}{x} dydx \overset{\text{fubini}}{=} \int_0^\infty\int_0^a e^{-xy}\frac{\sin x}{x} dxdy \\
        &= \int_0^\infty \frac{1}{1 + y^2} - \cos a \frac{e^{-ay}}{1 + y^2} - \sin a \frac{ye^{-ay}}{1+y^2} \\ &= \pi/2 - \cos a \int_0^\infty\frac{e^{-ay}}{1 + y^2}dy - \sin a \int_0^\infty \frac{ye^{-ay}}{1+y^2}dy
    \end{align*}
    This gives the desired inequality, namely
    \begin{align*}
        \abs{\int_0^a \frac{\sin x}{x} - \pi/2} \leq \abs{\cos a}\int_0^\infty e^{-ay}dy + \abs{\sin a} \int_0^\infty  ye^{-ay}dy \leq \frac{1}{a} + \frac{1}{a^2} \overset{a \geq 1}{\leq} \frac{2}{a}
    \end{align*} \qed

    \begin{pb}\textbf{(Durrett 3.3.1)}
        Let \(\varphi\) be the characteristic function of \(X\), then let \(Y\) be i.i.d. with \(-X\), so that
        \begin{align*}
            \varphi_{X+Y}(t) = E[e^{it(X+Y)}] \overset{\text{ind.}}{=} E[e^{itX}]E[e^{itY}] = \varphi(t)\overline{\varphi(t)} = \abs{\varphi(t)}^2
        \end{align*}

        To get that \(\Re \varphi\) is a characteristic function, note first that it must be
        \begin{align*}
            \Re \varphi(t) = E\left[\frac{1}{2}(e^{itX} + e^{-itX})\right]
        \end{align*}
        We can actually cheat here using Bochner's theorem with linearity, and \(\Re \varphi(0) = 1\), but using Bochner's theorem can feel like bad practice. Instead let \(Y\) be a random variable independent of \(X\) with \(P(Y = 1) = P(Y = -1) = \frac12\), then
        \begin{align*}
            \varphi_{YX}(t) &= E[e^{itYX}] = E[e^{itYX}(1_{Y=1} + 1_{Y=-1})] = E[e^{itX}1_{Y=1}] + E[e^{-itX}1_{Y=-1}] \\
            &\overset{\text{indep}}{=} \varphi(t)E[1_{Y=1}] + \overline{\varphi(t)}E[1_{Y=-1}] = \Re \varphi(t)
        \end{align*} \qed
    \end{pb}
    \begin{pb}\textbf{(Durrett 3.3.2)} \textbf{(i)}
        \begin{align*}
            \int_{-T}^T e^{-ita}\varphi(t)dt &= \int_{-T}^T \int e^{it(X-a)}d\mu dt \overset{\text{Fubini}}{=} \int\int_{-T}^T e^{it(X-a)}dtd\mu = \int\int_{-T}^T (\cos + i\sin)(t(X-a))dtd\mu \\
            &= \int\int_{-T}^T \cos(t(X-a))dtd\mu = \int \frac{2\sin(T(X-a))}{X-a}d\mu
        \end{align*}
        Fubini here is justified, since the integrand has absolute value \(\leq 1\), and \(\mu\) is a probability measure, now applying the limit we want to use DCT, which is justified by \(\frac{\sin(T(X-a))}{T(X-a)} \leq 1\).
        \begin{align*}
            \lim_{T \to \infty}\int\frac{2\sin(T(X-a))}{2T(X-a)}d\mu = \int\lim_{T\to\infty}\frac{\sin(T(X-a))}{T(X-a)}d\mu = \int1_{\set{a}}d\mu = \mu\set{a}
        \end{align*}

        \textbf{(ii)} If \(x \not \in h \mathbb{Z}\), then \(P(X = x) = 0\). Now assuming \(x \in h \mathbb{Z}\), we find that \(e^{-i t x}\) is \(\frac{2\pi}{h}\) periodic. Moreover
        \begin{align*}
            \varphi(t + 2\pi/h) &= E[\exp(iX(t + 2\pi/h))] = E[1_{h \mathbb{Z}}\exp(iX(t + 2\pi/h))] = \int_{h \mathbb{Z}}\exp(iX(t + 2\pi/h)) d\mu \\
            &= \int_{h \mathbb{Z}}e^{itX}e^{X2\pi/h}d\mu = \int_{h \mathbb{Z}} e^{itX} = \varphi(t)
        \end{align*}
        Now, using the previous subpart we find that
        \begin{align*}
            P(X = x) = \lim_{T \to \infty} \frac{1}{2T}\int_{-T}^T \exp(-i tx)\varphi(t)dt
        \end{align*}
        Since the limit exists, it suffices to examing a subsequence, using \(2\pi/h\) periodicity, we take \(T_n = \frac{2\pi n + \pi}{h}\)
        \begin{align*}
            P(X = x) = \lim_{n\to\infty} \frac{h}{2\pi(2n+1)}\int_{-T_n}^{T_n} exp(-i tx)\varphi(t)dt = \frac{h}{2\pi}\int_{-\pi/h}^{\pi/h} exp(-i tx)\varphi(t)dt
        \end{align*}

        \textbf{(iii)} The equality written in the statement is one of the consequences of the definition of characteristic functions. This result is striaghtforward.
    \end{pb}
    \begin{pb}\textbf{(Durrett 3.3.3)}
        \(\varphi_{X-Y} = \abs{\varphi}^2\), so by the previous problem, and noting \(\set{x \mid \mu\set{x} \neq 0}\) is countable,
        \begin{align*}
            \lim_{T\to\infty}\frac{1}{2T}\int_{-T}^T \abs{\varphi(t)}^2dt &= P(X-Y = 0) = \int{1_{X - Y = 0}}d\mu d\mu = \int \int 1_{Y=X} d\mu d\mu = \int \int 1_{Y = x}d \mu d\mu(x) \\
            &= \int \mu\set{x} d\mu = \int\mu\set{x}1_{\mu\set{x}\neq 0} = \sum_{\set{x \mid \mu\set{x} \neq 0}} \mu\set{x}\int 1_{\set{x}}d\mu = \sum_x \mu\set{x}^2
        \end{align*} \qed

        This exercise has some important consequences, namely if \(\varphi(t) \overset{t\to\infty}{\longrightarrow} 0\), then \(\mu\) does not have any point masses, moreover the converse is false by the next exercise (3.3.11). There is a partial converse, which says that if \(\mu\) has a density, the \(\varphi(t) \to 0\), to see this if \(\mu\) has a density \(f\), then \(\varphi(t) = \int e^{itx}f(x)dx\), and this is a consequence of the Riemann lebesgue lemma, where we show it for simple functions of intervals, then use that these are dense in \(L^1(\mathbb{R})\). On an interval, \(\abs{\int_a^b e^{itx}} = \leq 2/\abs{t}\). \qed
    \end{pb}
    \begin{pb}\textbf{Durrett 3.3.11}
        This problem provides an example of a random variable, with a continuous density, but not satisfying \(\lim_{t\to\infty}\varphi(t) = 0\), the random variable is defined using \(X_k \sim \text{Bernoulli}(1/2)\) and \(X = 2\sum_1^\infty X_j/3^j\). The distribution function is continuous, since it is the Cantor-Lebesgue function. Now we compute \(\varphi\), using exercise 3.3.9, the characteristic function for the Bernoulli distribution and \(\varphi_{aX}(t)=\varphi_X(at)\) we get
        \begin{align*}
            \varphi(t) = \prod_1^\infty \varphi_{2X_i/3^j}(t) = \prod_1^\infty \frac12\left(e^{it\frac{2}{3^j}} + 1\right)
        \end{align*}
        Now when \(t = 3^k\pi\), we get
        \begin{align*}
            \varphi(t) = \prod_1^\infty \frac{1}{2}\left(e^{it\frac{2\pi}{3^{j-k}}} + 1\right) = 1\cdot\prod_{k+1}^\infty \frac{1}{2}\left(e^{it\frac{2\pi}{3^{j-k}}} + 1\right) = \varphi(\pi)
        \end{align*}
        So as long as we can show \(\abs{\varphi(\pi)} > 0\), we are done. We first establish
        \begin{align*}
            \abs{e^{i2\pi/3^j} - 1} \leq \abs{\sin2\pi/3^j} + \abs{1 - \cos2\pi/3^j} \leq \frac{2\pi}{3^j} + \frac{2\pi^2}{3^{2j}} \overset{j>2}{\leq} \frac{4\pi}{3^j}
        \end{align*}
        Now taking \(C = \abs{\prod_1^{r-1} \frac12\left(e^{it\frac{2}{3^j}} + 1\right)}\), where \(r > 2\) is chosen so that \(2\pi\sum_r^\infty3^{-j} < \frac12\) we find that
        \begin{align*}
            \abs{\varphi(\pi)} \geq C\prod_r^\infty \frac12\left(2-\frac{4\pi}{3^j}\right) \geq C\prod_r^\infty 1 - \frac{2\pi}{3^j} \geq C - C2\pi\sum_r^\infty 1/3^j \geq C/2 > 0
        \end{align*} \qed
    \end{pb}
    \begin{pb}\textbf{(Durrett 3.3.4)}
        Take the distribution of a coinflip, with \(P(X=1)=P(X=-1)=\frac12\), then
        \begin{align*}
            \varphi(t) = E[e^{itX}] = \frac{1}{2}(e^{it} - e^{-it}) = \cos t
        \end{align*}
        \(\cos t \not \to 0\), so it is apparent that \(\int_{-\infty}^\infty \abs{\cos t} = \infty\). \qed
    \end{pb}
    \begin{pb}\textbf{(Durrett 3.3.6)}
        Firstly, to solve for the Cauchy distribution, we use that \(f(x) = \frac{1}{2\pi}\int e^{-itx}\varphi(t)dt\), in our case we find that \(\varphi = e^{-\abs{t}}\), since
        \begin{align*}
            \frac{2}{\pi}f(x) &= \frac{2}{1+x^2} = \frac{1}{1+ix} - \frac{1}{1-ix} = \int_0^\infty e^{-t(1+ix)}dt - \int_0^\infty e^{-t(1-ix)}dt \\
            &= \int_0^\infty e^{-t(1+ix)}dt + \int_{-\infty}^0 e^{-t(ix - 1)}dt = \int e^{-\abs{t}}e^{-itx}dt = \int \varphi(t)e^{-itx}dt
        \end{align*}
        Then we can use \(\varphi_{\frac{\sum_1^n X_k}{n}} = \prod_1^n \varphi_{\frac{X_k}{n}}\), and \(\varphi_{\frac{X_k}{n}}(t) = \varphi_{X_k}(t/n)\), so that
        \begin{align*}
            \varphi_{\frac{\sum_1^n X_k}{n}}(t) = \prod_1^n e^{-\abs{t}/n} = \exp\left(\sum_1^n-\abs{t}/n \right)= e^{-\abs{t}} = \varphi_{X_1}(t)
        \end{align*} \qed
    \end{pb}
    \begin{pb}\textbf{(Durrett 3.3.7)}
        \(X_n \implies X\), hence \(\varphi_n \to \varphi\), where \(\varphi_n = e^{-\frac12 \sigma_n^2t^2}\) and \(\varphi = e^{-\frac12 \sigma^2t^2}\), noting that \(\varphi_n/\varphi \to 1\), we have that \(e^{\frac12 t^2 (\sigma^2 - \sigma_n^2)} \to 1\) for all \(t\), now letting \(t = \sqrt{2}\), its apparent this only happens when \(\sigma_n \to \sigma\). To justify the characteristic functions,
        \begin{align*}
            E[\exp(itX)] = \frac{1}{\sqrt{2\pi \sigma^2}}\int \exp(itx - \frac{x^2}{2\sigma^2})dx = \frac{1}{\sqrt{2\pi \sigma^2}}\int \cos(tx)e^{-\frac{x^2}{2 \sigma^2}}dx
        \end{align*}
        Now defining \(\varphi(t) = \frac{1}{\sqrt{2\pi \sigma^2}}\int \cos(tx)e^{-\frac{x^2}{2 \sigma^2}}dx\), we can swap the derivative and integral since our function is absolutely integrable. This gives us
        \begin{align*}
            \varphi'(t) = \int -xe^{-\frac{x^2}{2 \sigma^2}}\sin tx dx
        \end{align*}
        Now integrating by parts with \(u = \sin tx, du = t\cos tx\), and \(dv = -xe^{-\frac{x^2}{2 \sigma^2}}\), we get \(v = \sigma^2 e^{-\frac{x^2}{2 \sigma^2}}\) so that
        \begin{align*}
            \varphi'(t) = - \sigma^2t \varphi(t) \implies \varphi(t) = e^{- \frac{\sigma^2 t^2}{2}}
        \end{align*} \qed
    \end{pb}
    \begin{pb}\textbf{(Durrett 3.3.8)}
        We get \(\varphi_n^X \to \varphi^X\) and \(\varphi_n^Y \to \varphi^Y\), then \(\varphi_n^{X+Y} = \varphi_n^X \varphi_n^Y \to \varphi^X \varphi^Y\).
    \end{pb}
    \begin{pb}\textbf{(Durrett 3.3.9)}
        For any \(n\), we get that
        \begin{align*}
            \varphi_{S_n} = \varphi_{S_{n-1} + X_n} = \varphi_{S_{n-1}}\varphi_{X_n}
        \end{align*}
        applying this inductively we get that \(\varphi_{S_n} = \prod_1^n \varphi_{X_n}\), since \(S_n \to S\), the characteristic functions \(\varphi_{S_n} \to \varphi_S\) pointwise, since \(S_n \implies S\). Then it follows that for each \(t\), \(\varphi_S(t) = \lim_{n\to\infty} \prod_1^n \varphi_{X_n}(t)\), and in particular the limit exists.
    \end{pb}
    \begin{pb}\textbf{(Durrett 3.3.12)}
        We first Taylor expand \(e^{-t^2/2}\),
        \begin{align*}
            e^{-t^2/2} = \sum_0^\infty (-1)^n\frac{t^{2n}}{n!2^n}
        \end{align*}
        Now taking \(X \sim N(0,1)\), and using the taylor expansion in the last equality, we find that
        \begin{align*}
            EX^{2n} = \frac{1}{\sqrt{2\pi}}\int x^{2n}e^{-x^2/2}dx = (-1)^n \varphi^{(2n)}(0) = (-1)^n\left.\left(\frac{d}{dt}\right)^{2n}\right\vert_0 e^{-t^2/2} = \frac{2n!}{2^n n!}
        \end{align*} \qed
    \end{pb}
    \begin{pb}\textbf{(Durrett 3.3.14)} \textbf{(i)}
        We can't use an \(L^2\) weak law because we only have access to information on the first two terms of the expansion, it will be hard to use a weak law in general due to only having the characteristic functions, so we need to show it directly, first noting
        \begin{align*}
            \lim_{n\to\infty} \frac{\varphi(t/n) - \varphi(0)}{t/n} = ia \implies n(\varphi(t/n) - 1) = iat
        \end{align*}
        we can rearrange to find that
        \begin{align*}
            \varphi_{S_n/n}(t) = \varphi(t/n)^n = \left(1 + n\frac{\varphi(t/n) - 1}{n}\right)^n
        \end{align*}
        So that \(\lim_{n\to\infty} \varphi_{S_n/n} = e^{iat} = \varphi_a\) by continuity of \(\log\), since \(e^{iat}\) is continuous at zero, we get weak convergence \(S_n/n \implies a\).

        \textbf{(ii)} Immediate consequence of the continuity theorem.

        \textbf{(iii)} \textcolor{red}{TODO}
    \end{pb}
    \begin{pb}\textbf{(Durrett 3.3.16)}
        If \(EX = 0\), and \(\var X = 0\), then \(EX^2 = 0\), which implies \(X \overset{L^2}{=} 0\), so that \(X = 0\) a.s.

        Now for the main result, we have from taylors theorem \(\lim_{h\to 0}\frac{1 - \cos hx}{h^2} = x^2\), this alongside Fatou's lemma gives us
        \begin{align*}
            E[X^2] &\overset{\text{Fatou}}{\leq} 2\liminf_{h\to 0}E\left[\frac{1-\cos(hX)}{h^2}\right] = 2\liminf_{h\to0}E\left[\frac{1}{h^2} - \frac{e^{ihx} + e^{-ihx}}{2h^2}\right] = \liminf_{h\to 0}\frac{2 - \varphi(h) - \varphi(-h)}{h^2} \\
            &= - \limsup_{h\to 0} \frac{\varphi(h) + \varphi(-h) - 2}{h^2} = -2\limsup_{h \to 0}\frac{\varphi(h)-1}{h^2} < \infty
        \end{align*}
        Then since \(E[X^2] < \infty\), we get that
        \begin{align*}
            \varphi(t) = 1 +itE[X] -t^2E[X^2] + o(t^2)
        \end{align*} so that \(E[X] = 0\) for the above limit to make sense. \qed
    \end{pb}
    \begin{pb}\textbf{(Durrett 3.3.17)}
        One direction is obvious from the continuity theorem, in the other direction \(\varphi_n \to \varphi\) where \(\varphi\) is continuous on \((-\delta,\delta)\) so once again by the continuity theorem, \(Y_n \implies Y\), where \(Y\) has characteristic function \(\varphi\), we want to show that \(\varphi = 1\), so we will be done by the inversion formula. Now since \(\lim_{t\downarrow 0}\frac{\varphi(t) - 1}{t^2} = 0 > -\infty\), we get from the proof of the previous exercise (3.3.16) \(E[Y^2] = 0\), and hence \(Y = 0\) a.s. \qed
    \end{pb}
    \begin{pb}\textbf{(Durrett 3.4.1)}
        The histogram correction in this case is \(P(S_{180} \leq 24.5)\), applying the approximation given by the CLT,
        \begin{align*}
            P(S_{180} \leq 24.5) = P\left(\frac{S_{180}-30}{5} \leq -11/10\right) = \Phi(-1.1) \approx 13.57
        \end{align*} \qed
    \end{pb}
    \begin{pb}\textbf{(Durrett 3.4.4)}
        We may apply the central limit theorem to conclude that \(\frac{S_n - n}{\sigma\sqrt{n}} \implies \chi\). Now we consider \(B_n = \sqrt{\frac{S_n}{n}} + 1\), then we can apply the Cauchy Schwartz inequality,
        \begin{align*}
            E\abs{X_i} \leq \left(EX_i^2\right)^{\frac12}P(\Omega) = \sigma < \infty
        \end{align*}
        so that we may apply the strong law of large numbers to \(\frac{S_n}{n} \overset{\text{a.s.}}{\longrightarrow} 1\), it follows that \(B_n \to 2\) a.s. and \(\frac{1}{B_n} \to \frac12\) a.s. which of course implies \(\frac{1}{B_n} \implies \frac12\), by the converging together lemma for products we get that
        \begin{align*}
            \frac{S_n - n}{\sigma\sqrt{n}}\frac{1}{B_n} \implies \frac{1}{2}\chi
        \end{align*}
        rewriting the left side gives us
        \begin{align*}
            \frac{(\sqrt{S_n}-\sqrt{n})(\sqrt{S_n}+\sqrt{n})}{\sigma\sqrt{n}}\frac{\sqrt{n}}{(\sqrt{S_n}+\sqrt{n})} \implies \frac12 \chi
        \end{align*}
        Cancelling and multiplying constants gives
        \begin{align*}
            2(\sqrt{S_n}-\sqrt{n}) \implies \sigma \chi
        \end{align*} \qed
    \end{pb}
    \begin{pb} \textbf{(Durrett 3.4.5)}
        Let \(L_n = \frac{\sigma\sqrt{n}}{\sqrt{\sum_1^n X_k^2}} \tand R_n = \frac{\sum_1^n X_k}{\sigma\sqrt{n}}\), then \(\frac{1}{L_n} \to 1\) a.s. by the strong law, which gives us \(L_n \to 1\) a.s. whence \(L_n \implies 1\) we also have \(R_n \implies \chi\) by the CLT, so by the converging together lemma \(L_n R_n \implies \chi\). We are done since
        \begin{align*}
            T_n = T_n \frac{\sigma\sqrt{n}}{\sigma\sqrt{n}} = L_nR_n
        \end{align*} \qed
    \end{pb}
    \begin{pb} \textbf{(Durrett 3.4.6)}
        Define \(Y_n = \frac{S_{N_n}}{\sigma\sqrt{a_n}}, Z_n = \frac{S_{a_n}}{\sigma\sqrt{a_n}}\), then since \(a_n \to \infty\), and \(\frac{S_n}{\sigma\sqrt{n}} \implies \chi\) by CLT, we get that \(Z_n \implies \chi\). We check now that \(Y_n - Z_n \overset{P}{\longrightarrow} 0\), since this will imply they also converge weakly to zero, then we can use the converging together lemma to get
        \begin{align*}
            Y_n = Z_n + Y_n - Z_n \implies \chi + 0 = \chi
        \end{align*}
        It remains to show convergence in probability, letting \(\epsilon > 0\), and choosing \(\delta = \frac{\epsilon^3}{2}\), we find that
        \begin{align*}
            P(\abs{Y_n - Z_n} > \epsilon) &= P(\abs{Y_n - Z_n} > \epsilon,\abs{1-\frac{N_n}{a_n}} < \delta) + P(\abs{Y_n - Z_n} > \epsilon,\abs{1-\frac{N_n}{a_n}} < \delta) \\
            &\leq P(\abs{Y_n - Z_n} > \epsilon,\abs{1-\frac{N_n}{a_n}} < \delta) + P(\abs{1-\frac{N_n}{a_n}} < \delta)
        \end{align*}
        The second term goes to zero by assumption, so it remains to bound the first term.
        \begin{align*}
            P(\abs{Y_n - Z_n} > \epsilon,\abs{1-\frac{N_n}{a_n}} < \delta) &\leq P\left(\max_{a_n - \delta \leq m \leq a_n + \delta}\abs{\frac{\sum_{\min\set{m,a_n}}^{\max\set{m,a_n}}X_k}{\sigma\sqrt{a_n}}} > \epsilon\right) \\
            &= P\left(\max_{a_n - \delta \leq m \leq a_n + \delta}\abs{\sum_{\min\set{m,a_n}}^{\max\set{m,a_n}}X_k} > \epsilon\sigma\sqrt{a_n}\right) \\
            &\overset{\text{max. ineq.}}{\leq} \frac{1}{\sigma^2 a_n \epsilon^2} \var\sum_{\lceil a_n(1-\delta)\rceil}^{\lfloor a_n(1+\delta)\rfloor} X_k \\
            &= \frac{1}{\sigma^2 a_n \epsilon^2} \abs{\lfloor a_n(1+\delta)\rfloor - \lceil a_n(1-\delta)\rceil}\sigma^2 \\
            &\leq \frac{a_n 2\delta \sigma^2}{\sigma^2 a_n \epsilon^2} = \epsilon
        \end{align*}
        So that \(\abs{X_n - Y_n} \overset{P}{\longrightarrow} 0\). \qed
    \end{pb}
    \begin{pb}\textbf{(Durrett 3.4.9)}
        The example shows why the assumption \(\sum_1^nE[X_{n,k}^21_{\abs{X_{n,k}}>e}] \to 0\) is essential in the LF-CLT.
        \begin{align*}
            \var(X_k) = \frac{2k^2}{2} + 2(\frac{1 - k^{-2}}{2}) = 2 - k^{-2}
        \end{align*}
        It follows that \(\var\left(\frac{S_n}{\sqrt{n}}\right) = \frac{1}{n}\sum_1^n 2 - k^{-2} \geq 2 - \frac{1}{n}\sum_1^\infty k^{-2}\) so by the squeeze theorem the quantity converges to \(2\), this satisfies condition \((i)\) of the LF-CLT for \(X_{n,k} = \frac{X_k}{\sqrt{n}}\), intuitively condition (ii) fails because of the mass at \(X_k = \pm k\), however, we don't need to check this explicitly since failure of the LF-CLT confirms our suspicions. Let \(Y_k = X_k1_{\abs{X_k} \leq 1}\) and \(Y_{n,k} = \frac{1}{\sqrt{n}}Y_k\). Then \(P(X_k \neq Y_k) = \frac{1}{k^2}\), so that Borel-Cantelli tells us \(P(X_k \neq Y_k \text{ i.o.}) = 0\), now defining \(Z_n = \sum_1^n Y_k\), we find that if \(x\) is such that \(X_k(x) \neq Y_k(x)\) for only finitely many \(k\), then letting \(N\) be an index larger than the last occurence,
        \begin{align*}
            \frac{\abs{S_n - Z_n}(x)}{\sqrt{n}} \leq \frac{1}{\sqrt{n}} \sum_1^N k^2 \overset{n\to\infty}{\longrightarrow} 0
        \end{align*}
        So that \(n^{-\frac12}(S_n - Z_n) \to 0\) a.s. (weak convergence of course follows from this as well). Then if we can show \(Z_n \implies \chi\), we will be done by the converging together lemma. Finally, we check that we can apply the Lindenberg-Feller CLT to \(Y_{n,k}\), we first check that \(\sum_1^n E[Y_{n,k}^21_{\abs{Y_{n,k}}>\epsilon}] \to 0\), but this is straightforward since \(\abs{Y_k} \leq 1\), so for \(\sqrt{n} > 1/\epsilon\) we get \(\sum_1^n E[Y_{n,k}^21_{\abs{Y_{n,k}}>\epsilon}] =  0\). Finally we check that \(\sum_1^n E[Y_{n,k}^2] \to 1\), which allows us to conclude that \(Z_n \implies 1\) by the Lindenberg-Feller CLT.
        \begin{align*}
            \sum_1^n E[Y_{n,k}^2] = \frac{1}{n}\left(n - \sum_1^n k^{-2}\right) \geq 1 - \frac{1}{n}\sum_1^\infty k^{-2} \to 1
        \end{align*}
        This sum is also clearly less than or equal to \(1\) for all \(n\), so were done by the squeeze theorem.
        \qed
    \end{pb}
    \begin{pb}\textbf{(Durrett 3.4.10)}
        Define \(X_{n,k} = \frac{1}{\sqrt{\var S_n}}(X_k - EX_k)\), we simply check the conditions for Lindenberg-Feller CLT.
        
        \textbf{(i)}
        \begin{align*}
            \sum_1^n EX_{n,k}^2 = \frac{1}{\var S_n}\sum_1^n \var(X_k) = \frac{\var S_n}{\var S_n} = 1
        \end{align*}

        \textbf{(ii)}
        Let \(\epsilon > 0\), since \(\lim_{n\to\infty}\var S_n = \infty\) we can choose \(N\) such that \(n \geq N\) implies that \(\var S_n \geq \left(\frac{2M}{\epsilon}\right)^2\), then for \(n \geq N\), we have
        \begin{align*}
            \abs{X_{n,k}} = \frac{1}{S_n^{\frac12}}\abs{X_k - E[X_k]} \leq \frac{1}{S_n^{1/2}}2M \leq \epsilon
        \end{align*}
        So that \(E[X_{n,k}^21_{\abs{X_{n,k}} > \epsilon}] = 0\). \qed
    \end{pb}
    \begin{pb}\textbf{(Durrett 3.4.11)}
        Once again, we check the conditions of the Lindenberg-Feller CLT with \(X_{n,k} = X_k/\sqrt{n}\).

        \textbf{(i)} \(\sum_1^n E[(X_k/\sqrt{n})^2] = \frac{1}{n}nEX_1^2 = 1\)

        \textbf{(ii)} We can use the Borel-Cantelli lemma by noticing for any \(\epsilon > 0\), the following is summable
        \begin{align*}
            P(\abs{X_n}/\sqrt{n} > \epsilon) \leq \frac{1}{(\epsilon\sqrt{n})^{2+\delta}}E\abs{X_n}^{2+\delta} \leq \frac{C \epsilon^{-2}}{n^{1+\delta/2}}
        \end{align*}
        Then \(P(\abs{X_n}/\sqrt{n} > \epsilon \text{ i.o.}) = 0\), so for some \(N\), we have \(n \geq N\) implies that \(\abs{X_n}/\sqrt{n} \leq \epsilon\) almost surely. It follows that for \(n \geq N\), we have
        \begin{align*}
            \sum_1^n E[X_k^2/\sqrt{n}1_{\abs{X_k}/\sqrt{n}>\epsilon}] = \sum_1^N E[X_k^2/\sqrt{n}1_{\abs{X_k}/\sqrt{n}>\epsilon}] \leq \frac{1}{n}\sum_1^N EX_k^2 = N/n \to 0
        \end{align*} \qed
    \end{pb}
    \begin{pb}\textbf{(Durrett 3.4.12 - Lyapunov's Theorem)}
        Once again, we check the conditions of the Lindenberg-Feller CLT with \(X_{n,k} = \frac{X_k - EX_k}{\alpha_n}\)

        \textbf{(i)}
        \begin{align*}
            \sum_1^n EX_{n,k}^2 = \frac{1}{\alpha_n^2}\sum_1^n\var X_k = \alpha_n^2/\alpha_n^2 = 1
        \end{align*}

        \textbf{(ii)}
        \begin{align*}
            \sum_1^n E[X_{n,k}^21_{\abs{X_{n,k}} > \epsilon}] &\leq \sum_1^n E\left[\frac{\abs{X_{n,k}}^\delta}{\epsilon^\delta}X_{n,k}^21_{\abs{X_{n,k}} > \epsilon}\right] \leq \frac{1}{\epsilon^\delta}\sum_1^n E\left[\abs{X_{n,k}}^{2+\delta}\right] \\ &= \frac{1}{\epsilon^\delta} \alpha_n^{-2-\delta}\sum_1^n E[\abs{X_k - EX_k}^{2+\delta}] \to 0
        \end{align*}
        The convergence of the above sequence is by assumption. \qed
    \end{pb}
    \begin{pb}\textbf{(Durrett 3.4.13)}
        \textbf{(a)}

        \textbf{(b)} We can simply check the conditions for the Lindenberg-Feller CLT with \(X_{n,k} = X_k/n^{(3-b)/2}\)

        \textbf{(i)}
        \begin{align*}
            \sum_1^n E[X_{n,k}^2] = n^{\beta - 3}\sum_1^n EX_k^2 = n^{\beta - 3}\sum_1^n k^{2-\beta} =: T_n
        \end{align*}
        Which is clearly between zero and one for all \(n\). We need to check now that it converges to some \(\sigma^2 \in (0,\infty)\), to do so, consider \(T_{n+1} - T_n\) (and denote \(C = \sum_1^\infty k^{2-\beta} < \infty\)), then
        \begin{align*}
            T_{n+1} - T_n &= \frac{1}{n+1} + ((n+1)^{\beta-3} - n^{\beta-3})\sum_1^nk^{2-\beta} \geq \frac{1}{n+1} + ((n+1)^{\beta-3} - n^{\beta-3})C \\
            & \geq \frac{1}{n+1} - Cn^{\beta-3}
        \end{align*}
        This is of course positive for sufficiently large \(n\), since \(\beta-3 > 1\), so the sequence \(T_n\) is bounded, and eventually increasing hence converges to some \(c \in [\inf\set{T_n}, 1]\) by Bolzano Weierstrass. The sequence being eventually increasing, and lying in \((0,1]\) also allows us to conclude \(c \geq \inf\set{T_n} > 0\).

        \textbf{(ii)} We have \[\abs{X_{n,k}} \leq \frac{k}{n^{(3-b)/2}} \leq \frac{n}{n^{(3-b)/2}} \leq n^{(\beta - 1)/2} \to 0\]
        So for sufficiently large \(n\), \(\abs{X_{n,k}} \leq \epsilon\) for any \(\epsilon > 0\).

        \textbf{(c)} Let \(t \in \mathbb{R}\), then consider for \(1 \leq k \leq n\), \(c_{n,k} := \frac{1 - \cos(tk/n)}{k/n}\), then since \(\abs{\frac{1 - \cos x}{x}} \leq \min\set{2/\abs{x},\frac{\abs{x}}{2}}\), we find that
        \begin{align*}
            \abs{c_{n,k}} = \abs{t}\abs{\frac{1 - \cos(tk/n)}{tk/n}} \leq \frac{k}{2n}\abs{t^2} \leq \frac{1}{2}\abs{t^2}
        \end{align*}
        Noting here that independent of \(t\), we always have \(\abs{c_{n,k}}/n \to 0\). Now still for fixed \(t\), consider
        \begin{align*}
            c := \int_0^1 x^{-1}(1-\cos xt)dx
        \end{align*}
        by a similar inequality to what we just showed, we know that the integrand is bounded on \([0,1]\) thus is Riemann integrable, moreover since the limit of the integrand as \(x \to 0\) exists, and it is clearly continuous away from \(0\), we find that the integrand is continuous, so that the integral is equal to the Riemann sum of any partition, so long as the mesh of the partition goes to zero. From this, we find that
        \begin{align*}
            \sum_1^n \frac{c_{n,k}}{n} \to c
        \end{align*}
        This method of obtaining the inequality unfortunately means we will not be able to use the technique from CLT of writing \(\abs{\prod_1^n z_k - \prod_1^n \omega_k} \leq \theta^{n-1}\sum_1^n \abs{z_k-\omega_k}\) since we havent shown \(c_{n,k} \to c\), fortunately we can apply a similar proof specific to the real case. Considering \(n\) be large enough so that \(\abs{c_{n,k}}/n < \frac12\)
        \begin{align*}
            \abs{-c - \sum_1^n \log(1-\frac{c_{n,k}}{n})} = \abs{-c + \sum_1^n \frac{c_{n,k}}{n} + \sum_1^n \eta_{n,k}\frac{c_{n,k}^2}{n^2}}
        \end{align*}
        Here \(\abs{\eta_{n,k}} \leq \sup_{x \in [\frac{1}{2},\frac32]}\frac{1}{2}\frac{1}{(1-x)^2} =: \eta < \infty\) where this estimate on \(\eta_{n,k}\) and the above equality follow from Taylor's theorem.
        It follows that
        \begin{align*}
            \abs{-c - \sum_1^n \log(1-\frac{c_{n,k}}{n})} &\leq \abs{c - \sum_1^n \frac{c_{n,k}}{n}} + \frac{\eta}{n}\abs{\sum_1^n c_{n,k}^2/n} \leq \abs{c - \sum_1^n \frac{c_{n,k}}{n}} + \frac{\eta}{n} \sum_1^n \frac{1}{4n}\abs{t}^4 \\
            &\leq \abs{c - \sum_1^n \frac{c_{n,k}}{n}} + \frac{1}{n}\eta\abs{t}^4
        \end{align*}
        Which goes to zero as \(n \to \infty\). It follows that by continuity of exponential, we get \(\prod_1^n 1 - \frac{c_{n,k}}{n} \to e^{-c}\).

        Now denote the characteristic function of \(\frac{S_n}{n}\) as \(\varphi_n\), and the characteristic functions of \(X_k \tand \frac{X_k}{n}\) as \(\varphi_k \tand \varphi_{n,k}\) respectively, then
        \begin{align*}
            \varphi_n(t) &= \prod_1^n \varphi_{n,k}(t) = \prod_1^n \varphi_k(t/n) = \prod_1^n E[\exp(itX_k)] = \prod_1^n 1 - k^{-1} + k^{-1}\cos(tk/n) \\
            &= \prod_1^n 1 - k^{-1}\left(1-\cos(tk/n)\right) = \prod_1^n 1-n^{-1}\left(\frac{1-\cos(tk/n)}{k/n}\right) = \prod_1^n 1 - \frac{c_{n,k}}{n}
        \end{align*}
        Now we will be done once we show \(\varphi(t) = \exp\left(\int_0^1 x^{-1}(1-\cos xt)dx\right)\) is continuous at \(t=0\) by the continuity lemma. It suffices to check that \(\int_0^1 x^{-1}(1-\cos xt)dx\) is since \(e\) is, this is easy to see by the DCT, since at \(t = 0\) the integrand is zero, we write
        \begin{align*}
            \abs{x^{-1}(1-\cos xt)} = \abs{t} \abs{(tx)^{-1}(1-\cos xt)} \leq \abs{tx}^2/2
        \end{align*}
        so restricting to \(\abs{t} \leq 1\) we get the bounded convergence theorem with \(\frac12\) dominating the function, so that \(\lim_{t\to0}\int_0^1 x^{-1}(1-\cos xt)dx = \int_0^1\lim_{t\to0}x^{-1}(1-\cos xt)dx = 0\), this gives us that \(\varphi(t)\) is continuous at zero, and hence by the continuity theorem \(S_n/n \to \aleph\). \qed
    \end{pb}
\end{document}